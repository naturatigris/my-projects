This log analysis is carried out to find the most frequently logged users and also for finding the frequency of logs which could determine if there is any fault in the system or server.
Logs typically contain time-series data that is either streamed using collectors real-time or stored for review at a later time. Log analysis offers insight into system performance and can indicate possible problems such as security breaches or imminent hardware failure.
Generally, log data is collected for the log analysis program, cleansed, structured or normalized and then offered for analysis for the experts to detect patterns or uncover anomalies such as a cyber-attack or data exfiltration. Performing log file analysis generally follows these steps:
Data collection: Data from hardware and software probes is collected to a central database
 Data indexing: Data from all sources is centralize and indexed to speed searchability, enhancing IT professionals ability to rapidly uncover problems or patterns
 Analysis: Log analysis tools including normalization, pattern recognition, correlation, and tagging can be done either automatically using machine learning tools or manually where needed.
 Monitoring: A Real-time, autonomous log analysis platform can generate alerts when anomalies are detected. This type of automated log analysis is the underpinning for most continuous monitoring of the full IT stack
 Reports: Both traditional reports and dashboards are part of a log analysis platform, providing either at-a-glance or historical views of metrics for operations, development, and management stakeholders
